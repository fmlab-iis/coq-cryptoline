proc main(uint128 a0_0, uint128 a1_0, uint128 a2_0, uint128 a3_0, uint128 b0_0, uint128 b1_0, uint128 b2_0, uint128 b3_0) =
{ true && and [a0_0 <u 649037107316853453566312041152512@128, a1_0 <u 649037107316853453566312041152512@128, a2_0 <u 649037107316853453566312041152512@128, a3_0 <u 649037107316853453566312041152512@128, b0_0 <u 649037107316853453566312041152512@128, b1_0 <u 649037107316853453566312041152512@128, b2_0 <u 649037107316853453566312041152512@128, b3_0 <u 649037107316853453566312041152512@128] }
add v99_1 a3_0 18446744069414584320@uint128;
split v101_1 tmp_to_use_1 a2_0 64;
add v102_1 v99_1 v101_1;
and v103_1@uint128 a2_0 18446744073709551615@uint128;
assume v103_1 = tmp_to_use_1 && true;
add v104_1 v103_1 18446673704965373952@uint128;
add v106_1 a0_0 18446744073709551615@uint128;
add v108_1 a1_0 1298074214633706907132628377272319@uint128;
split v109_1 tmp_to_use_2 v102_1 64;
vpc a110_1@uint64 v109_1;
and v111_1@uint128 v102_1 18446744073709551615@uint128;
assume v111_1 = tmp_to_use_2 && true;
split tmp1_1 tmp2_1 v109_1 96;
shl v113_1 tmp2_1 32;
assume tmp1_1 = 0 && true;
sub v179_1 v113_1 v109_1;
add v114_1 v111_1 v179_1;
split v115_1 tmp_to_use_3 v114_1 64;
vpc a116_1@uint64 v115_1;
add b117_1 a110_1 a116_1;
and v118_1@uint128 v114_1 18446744073709551615@uint128;
assume v118_1 = tmp_to_use_3 && true;
split tmp1_2 tmp2_2 v115_1 96;
shl v120_1 tmp2_2 32;
assume tmp1_2 = 0 && true;
sub v178_1 v120_1 v115_1;
add v121_1 v118_1 v178_1;
vpc v122_1@uint128 b117_1;
add v123_1 v106_1 v122_1;
split tmp1_3 tmp2_3 v122_1 96;
shl v124_1 tmp2_3 32;
assume tmp1_3 = 0 && true;
sub v125_1 v108_1 v124_1;
split v126_1 tmp_to_use_4 v121_1 64;
vpc high127_1@uint64 v126_1;
subb high_1 high128_1 0@uint64 high127_1;
cast low129_1@uint64 v121_1;
vpc tmp_to_use_p_1@uint64 tmp_to_use_4;
assume low129_1 = tmp_to_use_4 && true;
cast v130_1@int64 v121_1;
assume v130_1 = low129_1 && true;
split low_h1bit_1 low_l63bit_1 low129_1 63;
vpc mask_1@uint1 low_h1bit_1;
and low133_1@uint64 low129_1 9223372036854775807@uint64;
adds discarded_1 low134_1 low133_1 9223372041149743103@uint64;
not low135_1@uint64 low134_1;
split low_1 discarded_2 low135_1 63;
vpc low_2@uint1 low_1;
cmov v139_1 mask_1 low_2 0@uint1;
cmov mask140_1 v139_1 1@uint1 high_1;
cmov v141_1 mask140_1 18446744073709551615@uint128 0@uint128;
sub v142_1 v123_1 v141_1;
cmov v143_1 mask140_1 4294967295@uint64 0@uint64;
vpc v144_1@uint128 v143_1;
sub v145_1 v125_1 v144_1;
cmov v146_1 mask140_1 18446744069414584321@uint64 0@uint64;
vpc v147_1@uint128 v146_1;
sub v148_1 v121_1 v147_1;
split v149_1 tmp_to_use_v142_1 v142_1 64;
add v150_1 v145_1 v149_1;
split v152_1 tmp_to_use_v150_1 v150_1 64;
add v153_1 v104_1 v152_1;
split v155_1 tmp_to_use_v153_1 v153_1 64;
add v156_1 v148_1 v155_1;
join value_1 0@uint64 18446744069414584320@uint64;
add v3_1 b3_0 value_1;
split v5_1 tmp_to_use_5 b2_0 64;
add v6_1 v3_1 v5_1;
join value_2 0@uint64 18446744073709551615@uint64;
and v1_1@uint128 b2_0 value_2;
assume v1_1 = tmp_to_use_5 && true;
join value_3 0@uint64 18446673704965373952@uint64;
add v8_1 v1_1 value_3;
join value_4 0@uint64 18446744073709551615@uint64;
add v11_1 b0_0 value_4;
join value_5 70368744177664@uint64 4294967295@uint64;
add v13_1 b1_0 value_5;
split v14_1 tmp_to_use_6 v6_1 64;
vpc a52_1@uint64 v14_1;
join value_6 0@uint64 18446744073709551615@uint64;
and v7_1@uint128 v6_1 value_6;
assume v7_1 = tmp_to_use_6 && true;
split tmp1_4 tmp2_4 v14_1 96;
shl v16_1 tmp2_4 32;
assume tmp1_4 = 0 && true;
sub v76_1 v16_1 v14_1;
add v17_1 v7_1 v76_1;
split v18_1 tmp_to_use_7 v17_1 64;
vpc a53_1@uint64 v18_1;
add b54_1 a52_1 a53_1;
join value_7 0@uint64 18446744073709551615@uint64;
and v9_1@uint128 v17_1 value_7;
assume v9_1 = tmp_to_use_7 && true;
split tmp1_5 tmp2_5 v18_1 96;
shl v20_1 tmp2_5 32;
assume tmp1_5 = 0 && true;
sub v75_1 v20_1 v18_1;
add v21_1 v9_1 v75_1;
vpc v22_1@uint128 b54_1;
add v23_1 v11_1 v22_1;
split tmp1_6 tmp2_6 v22_1 96;
shl v24_1 tmp2_6 32;
assume tmp1_6 = 0 && true;
sub v25_1 v13_1 v24_1;
split v26_1 tmp_to_use_8 v21_1 64;
vpc high55_1@uint64 v26_1;
subb high_2 high56_1 0@uint64 high55_1;
cast low57_1@uint64 v21_1;
vpc tmp_to_use_p_2@uint64 tmp_to_use_8;
assume low57_1 = tmp_to_use_8 && true;
cast v27_1@int64 v21_1;
assume v27_1 = low57_1 && true;
split low_h1bit_2 low_l63bit_2 low57_1 63;
vpc mask_2@uint1 low_h1bit_2;
and low59_1@uint64 low57_1 9223372036854775807@uint64;
adds discarded_3 low60_1 low59_1 9223372041149743103@uint64;
not low61_1@uint64 low60_1;
split low_3 discarded_4 low61_1 63;
vpc low_4@uint1 low_3;
cmov v31_1 mask_2 low_4 0@uint1;
cmov mask63_1 v31_1 1@uint1 high_2;
cmov v32_1 mask63_1 18446744073709551615@uint128 0@uint128;
sub v33_1 v23_1 v32_1;
cmov v34_1 mask63_1 4294967295@uint64 0@uint64;
vpc v35_1@uint128 v34_1;
sub v36_1 v25_1 v35_1;
cmov v37_1 mask63_1 18446744069414584321@uint64 0@uint64;
vpc v38_1@uint128 v37_1;
sub v39_1 v21_1 v38_1;
split v40_1 tmp_to_use_9 v33_1 64;
add v41_1 v36_1 v40_1;
cast v42_1@uint64 v33_1;
vpc tmp_to_use_p_3@uint64 tmp_to_use_9;
assume v42_1 = tmp_to_use_p_3 && true;
split v43_1 tmp_to_use_10 v41_1 64;
add v44_1 v8_1 v43_1;
cast v45_1@uint64 v41_1;
vpc tmp_to_use_p_4@uint64 tmp_to_use_10;
assume v45_1 = tmp_to_use_10 && true;
split v46_1 tmp_to_use_11 v44_1 64;
add v47_1 v39_1 v46_1;
cast v48_1@uint64 v44_1;
vpc tmp_to_use_p_5@uint64 tmp_to_use_11;
assume v48_1 = tmp_to_use_11 && true;
vpc v49_1@uint64 v47_1;
and v159_1@uint128 v142_1 18446744073709551615@uint128;
assume v159_1 = tmp_to_use_v142_1 && true;
vpc v13_2@uint128 v42_1;
mul a14_1 v13_2 v159_1;
split v15_1 tmp_to_use_12 a14_1 64;
and v16_2@uint128 a14_1 18446744073709551615@uint128;
assume v16_2 = tmp_to_use_12 && true;
vpc v18_2@uint128 v45_1;
mul a19_1 v18_2 v159_1;
split v20_2 tmp_to_use_13 a19_1 64;
and v21_2@uint128 a19_1 18446744073709551615@uint128;
assume v21_2 = tmp_to_use_13 && true;
add v22_2 v15_1 v21_2;
and v160_1@uint128 v150_1 18446744073709551615@uint128;
assume v160_1 = tmp_to_use_v150_1 && true;
mul a25_1 v13_2 v160_1;
split v26_2 tmp_to_use_14 a25_1 64;
and v27_2@uint128 a25_1 18446744073709551615@uint128;
assume v27_2 = tmp_to_use_14 && true;
add v28_1 v22_2 v27_2;
vpc v31_2@uint128 v48_1;
mul a32_1 v31_2 v159_1;
split v33_2 tmp_to_use_15 a32_1 64;
and v34_2@uint128 a32_1 18446744073709551615@uint128;
assume v34_2 = tmp_to_use_15 && true;
add v177_1 v20_2 v34_2;
add v35_2 v26_2 v177_1;
mul a36_1 v18_2 v160_1;
split v37_2 tmp_to_use_16 a36_1 64;
and v38_2@uint128 a36_1 18446744073709551615@uint128;
assume v38_2 = tmp_to_use_16 && true;
add v39_2 v35_2 v38_2;
and v161_1@uint128 v153_1 18446744073709551615@uint128;
assume v161_1 = tmp_to_use_v153_1 && true;
mul a43_1 v13_2 v161_1;
split v44_2 tmp_to_use_17 a43_1 64;
and v45_2@uint128 a43_1 18446744073709551615@uint128;
assume v45_2 = tmp_to_use_17 && true;
add v46_2 v39_2 v45_2;
vpc v49_2@uint128 v49_1;
mul a50_1 v49_2 v159_1;
split v51_1 tmp_to_use_18 a50_1 64;
and v52_1@uint128 a50_1 18446744073709551615@uint128;
assume v52_1 = tmp_to_use_18 && true;
add v174_1 v33_2 v52_1;
add v175_1 v37_2 v174_1;
mul a54_1 v31_2 v160_1;
split v55_1 tmp_to_use_19 a54_1 64;
and v56_1@uint128 a54_1 18446744073709551615@uint128;
assume v56_1 = tmp_to_use_19 && true;
add v176_1 v56_1 v175_1;
add v57_1 v44_2 v176_1;
mul a59_1 v18_2 v161_1;
split v60_1 tmp_to_use_20 a59_1 64;
and v61_1@uint128 a59_1 18446744073709551615@uint128;
assume v61_1 = tmp_to_use_20 && true;
add v62_1 v57_1 v61_1;
and v162_1@uint128 v156_1 18446744073709551615@uint128;
assume v162_1 = v156_1 && true;
mul a66_1 v13_2 v162_1;
split v67_1 tmp_to_use_21 a66_1 64;
and v68_1@uint128 a66_1 18446744073709551615@uint128;
assume v68_1 = tmp_to_use_21 && true;
add v69_1 v62_1 v68_1;
mul a71_1 v49_2 v160_1;
split v72_1 tmp_to_use_22 a71_1 64;
and v73_1@uint128 a71_1 18446744073709551615@uint128;
assume v73_1 = tmp_to_use_22 && true;
add v7_2 v55_1 v73_1;
add v171_1 v7_2 v51_1;
add v172_1 v60_1 v171_1;
mul a75_1 v31_2 v161_1;
split v76_2 tmp_to_use_23 a75_1 64;
and v77_1@uint128 a75_1 18446744073709551615@uint128;
assume v77_1 = tmp_to_use_23 && true;
add v173_1 v77_1 v172_1;
add v78_1 v67_1 v173_1;
mul a80_1 v18_2 v162_1;
split v81_1 tmp_to_use_24 a80_1 64;
and v82_1@uint128 a80_1 18446744073709551615@uint128;
assume v82_1 = tmp_to_use_24 && true;
add v83_1 v78_1 v82_1;
mul a85_1 v49_2 v161_1;
split v86_1 tmp_to_use_25 a85_1 64;
and v87_1@uint128 a85_1 18446744073709551615@uint128;
assume v87_1 = tmp_to_use_25 && true;
add v64_1 v76_2 v87_1;
add v3_2 v64_1 v72_1;
add v88_1 v3_2 v81_1;
mul a89_1 v31_2 v162_1;
split v90_1 tmp_to_use_26 a89_1 64;
and v91_1@uint128 a89_1 18446744073709551615@uint128;
assume v91_1 = tmp_to_use_26 && true;
add v92_1 v88_1 v91_1;
mul a94_1 v49_2 v162_1;
split v95_1 tmp_to_use_27 a94_1 64;
and v96_1@uint128 a94_1 18446744073709551615@uint128;
assume v96_1 = tmp_to_use_27 && true;
add v41_2 v90_1 v96_1;
add v97_1 v41_2 v86_1;
{ v16_2 + (v28_1 * 18446744073709551616) + (v46_2 * 340282366920938463463374607431768211456) + (v69_1 * 6277101735386680763835789423207666416102355444464034512896) + (v83_1 * 115792089237316195423570985008687907853269984665640564039457584007913129639936) + (v92_1 * 2135987035920910082395021706169552114602704522356652769947041607822219725780640550022962086936576) + (v97_1 * 39402006196394479212279040100143613805079739270465446667948293404245721771497210611414266254884915640806627990306816) + (v95_1 * 726838724295606890549323807888004534353641360687318060281490199180639288113397923326191050713763565560762521606266177933534601628614656) = (a0_0 + (a1_0 * 18446744073709551616) + (a2_0 * 340282366920938463463374607431768211456) + (a3_0 * 6277101735386680763835789423207666416102355444464034512896)) * (b0_0 + (b1_0 * 18446744073709551616) + (b2_0 * 340282366920938463463374607431768211456) + (b3_0 * 6277101735386680763835789423207666416102355444464034512896)) (mod 18446744073709551615 + (4294967295 * 18446744073709551616) + (0 * 340282366920938463463374607431768211456) + (18446744069414584321 * 6277101735386680763835789423207666416102355444464034512896)) && and [v103_1 = tmp_to_use_1, v111_1 = tmp_to_use_2, tmp1_1 = 0@128, v118_1 = tmp_to_use_3, tmp1_2 = 0@128, tmp1_3 = 0@128, low129_1 = tmp_to_use_p_1, v130_1 = low129_1, v1_1 = tmp_to_use_5, v7_1 = tmp_to_use_6, tmp1_4 = 0@128, v9_1 = tmp_to_use_7, tmp1_5 = 0@128, tmp1_6 = 0@128, low57_1 = tmp_to_use_p_2, v27_1 = low57_1, v42_1 = tmp_to_use_p_3, v45_1 = tmp_to_use_p_4, v48_1 = tmp_to_use_p_5, v159_1 = tmp_to_use_v142_1, v16_2 = tmp_to_use_12, v21_2 = tmp_to_use_13, v160_1 = tmp_to_use_v150_1, v27_2 = tmp_to_use_14, v34_2 = tmp_to_use_15, v38_2 = tmp_to_use_16, v161_1 = tmp_to_use_v153_1, v45_2 = tmp_to_use_17, v52_1 = tmp_to_use_18, v56_1 = tmp_to_use_19, v61_1 = tmp_to_use_20, v162_1 = v156_1, v68_1 = tmp_to_use_21, v73_1 = tmp_to_use_22, v77_1 = tmp_to_use_23, v82_1 = tmp_to_use_24, v87_1 = tmp_to_use_25, v91_1 = tmp_to_use_26, v96_1 = tmp_to_use_27, v16_2 <u 129127208515966861312@128, v28_1 <u 129127208515966861312@128, v46_2 <u 129127208515966861312@128, v69_1 <u 129127208515966861312@128, v83_1 <u 129127208515966861312@128, v92_1 <u 129127208515966861312@128, v97_1 <u 129127208515966861312@128, v95_1 <u 129127208515966861312@128] }